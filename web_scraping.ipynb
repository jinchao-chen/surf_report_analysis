{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# website scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import dateparser\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do list:\n",
    "\n",
    "\n",
    "-~~ if contains class=\"more-link\", redirect the link and use the link to update the post~~ \n",
    "\n",
    "- factorize the functions, and use a class to \n",
    "- search for report date based on the post date + day of the week (more flexible)\n",
    "- improve the doc-string quality\n",
    "- find a way to split the wetsuit info for shoe, wetsuit,glove and hoed \n",
    "- exception handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_posts(url):\n",
    "    \"\"\"\n",
    "    extract all the posts on each webpage\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    posts = soup.find_all(id=re.compile(\"post\"))\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def forecast_parser(post):\n",
    "    \"\"\"\n",
    "    loop through each paragrah using the headings contained in the tag <strong>\n",
    "    \"\"\"\n",
    "    tags = post.find_all('strong')\n",
    "\n",
    "    dates = []\n",
    "    contents = []\n",
    "    days_of_week = []\n",
    "\n",
    "    for tag in tags:\n",
    "        # dates needs to be corrected using the year specified in the post\n",
    "        date = dateparser.parse(tag.get_text())\n",
    "        dayofweek = tag.get_text().split()[0].lower()\n",
    "        dates.append(date)\n",
    "        days_of_week.append(dayofweek)\n",
    "\n",
    "        content = tag.parent.get_text()\n",
    "        contents.append(content)\n",
    "\n",
    "    return dates, contents, days_of_week\n",
    "\n",
    "\n",
    "def post_to_df(post):\n",
    "    \"\"\"\n",
    "    extract data from each post, write to dataframe \n",
    "    \"\"\"\n",
    "    post_id = post['id']\n",
    "    post_title = post.find_all('h2')[0].get_text()\n",
    "    post_date = dateparser.parse(' '.join(post_title.split()[1::]))\n",
    "    \n",
    "    forecasts = post.find_all(id = 'surf-weerbericht')[0]\n",
    "    forecast_dates, forecast_contents, days_of_week = forecast_parser(forecasts)\n",
    "    \n",
    "    df = pd.DataFrame({'report_date': forecast_dates,\n",
    "                       'report_content': forecast_contents,\n",
    "                       'day_of_week': days_of_week})\n",
    "\n",
    "    wetsuit_adviezen_str = wetsuit_recommendation(forecasts)\n",
    "\n",
    "    df['post_date'] = post_date  # year need to be udpated\n",
    "    df['post_id'] = post_id\n",
    "    df['post_title'] = post_title\n",
    "    df['wetsuit_adviezen_str'] = wetsuit_adviezen_str\n",
    "    df['wetsuit_adviezen'] = df['day_of_week']\n",
    "    df = df.replace({\"weekend\", 'zaterdag'})\n",
    "\n",
    "    try:\n",
    "#         wetsuit_adviezen_str = wetsuit_adviezen_str.get()\n",
    "        wetsuit_adviezen_dict = str_to_dict(wetsuit_adviezen_str)\n",
    "        df.replace({\"wetsuit_adviezen\": wetsuit_adviezen_dict}, inplace=True)\n",
    "\n",
    "    except:\n",
    "        df['wetsuit_adviezen'] = None\n",
    "\n",
    "    return df\n",
    "\n",
    "def wetsuit_recommendation(forecasts):\n",
    "    \"\"\"\n",
    "    get wetsuit recommendation\n",
    "    \"\"\"\n",
    "    wetsuit_adviezen = None\n",
    "    \n",
    "    for content in forecasts.find_all('p'):\n",
    "\n",
    "        text = re.sub(r'\\W+', ' ', content.get_text()).lower()\n",
    "        key_words = ['wetsuit adviezen', 'Wetsuit advies', \"Wetsuit mm advies\", 'Wetsuit diktes']\n",
    "\n",
    "        if any(word.lower() in text for word in key_words):\n",
    "#             print('a match')\n",
    "            wetsuit_adviezen = content\n",
    "            return wetsuit_adviezen.get_text()\n",
    "        \n",
    "    return wetsuit_adviezen\n",
    "\n",
    "\n",
    "def str_to_dict(wetsuit_adviezen):\n",
    "    \"\"\"\n",
    "    convert wetsuit_adviezen to a dictionary that uses dayofweek as the key\n",
    "    \"\"\"\n",
    "\n",
    "    # ['Tests run: 1', ' Failures: 0', ' Errors: 0']\n",
    "    a = wetsuit_adviezen.split('\\n')\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    for b in a:\n",
    "        try:\n",
    "            i = b.split(': ')\n",
    "            d[i[0].lower()] = i[1]\n",
    "        except:\n",
    "            d[i[0].lower()] = None\n",
    "            \n",
    "    d = update_weekend(d)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def update_weekend(d): \n",
    "    \n",
    "    # broadcast the values assigned to weekend, to Friday, saterday and sunday   \n",
    "    if 'weekend' in d.keys():\n",
    "        d['zaterdag'] = d['weekend']\n",
    "        d['zondag'] = d['weekend']\n",
    "        \n",
    "        if 'vrijdag' not in d.keys():\n",
    "            d['vrijdag'] = d['weekend']\n",
    "    \n",
    "    return d\n",
    "\n",
    "def get_href(url):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    hrefs = soup.find_all('a', text=re.compile('Surfweer*'), rel = 'bookmark')\n",
    "    return hrefs\n",
    "\n",
    "def get_post(post_url):\n",
    "    \"\"\"\n",
    "    get \n",
    "    \"\"\"\n",
    "    page = requests.get(post_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    post = soup.find('div', class_ = \"post\")\n",
    "    \n",
    "    return post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test replace with dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " correct the meer info issue  (some posts were not completely shown)\n",
    "- to resolve this issue, i will first read the hyper-link \n",
    "- use the hyper-link to retrieve the information \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_urls = []\n",
    "page_numbers = np.arange(10) + 1\n",
    "\n",
    "for page_number in page_numbers:\n",
    "    \n",
    "    page_url = 'http://surfweer.nl/surf/page/{:}/'.format(page_number)\n",
    "    hrefs = get_href(page_url)\n",
    "    post_urls.append([href.get('href') for href in hrefs])\n",
    "    \n",
    "post_urls = sum(post_urls, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for post_url in post_urls:\n",
    "    post = get_post(post_url) \n",
    "    df = post_to_df(post) \n",
    "    df['post_url'] = post_url\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_combined = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wetsuit advies deze week\\nMaandag: 6/5/4mm wetsuit, want 3mm, schoen 6mm, cap 2mm\\nDinsdag: Nieuwe 5/4mm wetsuit, schoen 6mm en want 3mm, cap 2mm\\nWoensdag: wetsuit 6mm of 5/4mm cap +1mm, hand 5mm, schoen 5mm\\nDonderdag: Min. 5/4mm dikte schoen 6mm, hand 4mm, cap 3mm'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.loc[4,'wetsuit_adviezen_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2021-01-26\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_combined.to_csv('surfweer_data_{:}.csv'.format(str(today).replace('-','_')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
